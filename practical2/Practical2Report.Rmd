---
title: "Practical Report 2"
output:
   pdf_document: 
     keep_tex: true
     fig_caption: yes
     highlight: monochrome
   html_document:
    df_print: paged
    code_folding: hide
fontsize: 12pt
---

```{r setup, include = FALSE}
require(knitr)
pdf_doc <- knitr::is_latex_output()
if (pdf_doc) {opts_chunk$set(echo=FALSE, fig.align="center", out.width="70%")} else {opts_chunk$set(echo=TRUE)}
```

# Acceptable residual plots

In this practical we will explore simulated data to illustrate residual diagnostics which are part of the linear modelling process. Residual plots allows us to judge the appropriateness of the assumption of linearity and homoscedasticity.

## Section 1, Question 1

```{r, Data1}
# Set a seed to make simulations reproducible
set.seed(2302) 
# Dataset 1
x <- 1:100
y <- 3 + 0.5*x + rnorm(100, 0, 2)
```

Linear model: $$Y_j \quad = \quad  3 + \frac{1}{2}x_j + \epsilon_j, \qquad j=1,\ldots, 100,$$ where $\epsilon_1, \ldots, \epsilon_{100}$ are iid $N(0, 4)$. (Remark: `rnorm` takes standard deviation as the argument, see in `?rnorm`.)

## Section 1, Question 2

```{r, SLR1, fig.cap=""}
# Fit a simple linear regression
model1 <- lm(y ~ x)
# Scatterplot with line of best fit
plot(x, y)
abline(model1, col="blue")
# Coefficients of fitted model
coef(model1)
```

We can see that the estimated intercept and slope are reasonably close to the true parameter, 3 and 0.5. We can double-check this by the 95% confidence intervals.

```{r CI_model1}
confint(model1)
```

## Section 1, Question 3

```{r Residual1, fig.cap=""}
# Residual plot for model 1
plot(model1, 1)
```

The residuals scatter around the zero horizontal line and the variation of the residuals appears relatively constant. This implies no evidence of violations against the assumptions.

## Section 1 Question 4

Next, we sample Dataset 2 with only 20 data points.

```{r, LinearModel2, fig.show="hold", out.width="49%", fig.cap="Left: Right:"}
# Dataset 2
set.seed(1184) 
x2 <- 1:20
y2 <- 3 + 0.5*x2 + rnorm(20, 0, 1)
# Fit linear model
model2 <- lm(y2 ~ x2)
# Produce scatterplot with line of best fit
plot(y2 ~ x2)
abline(model2, col="blue")
# Produce residual plot 
plot(model2, 1) 
```

Harder to tell if satisfying the model assumptions. We can see some curvature and uneven variance of the residuals even if the data is sampled from a linear model.

Question to think: how the confidence intervals change?

```{r CI_model2}
confint(model2)
```

```{r compare_CI}
print(paste0("model2 intercept CI length: ", confint(model2)[1, 2] - confint(model2)[1, 1]))
print(paste0("model2 slop CI length: ", confint(model2)[2, 2] - confint(model2)[2, 1]))
print(paste0("model1 intercept CI length: ", confint(model1)[1, 2] - confint(model1)[1, 1]))
print(paste0("model1 slop CI length: ", confint(model1)[2, 2] - confint(model1)[2, 1]))
```

# Unacceptable residual plots

The figure below shows the scatterplot of Dataset 3, a dataset sampled from the model $$ Y_j \quad = \quad 10 + \frac{1}{2}x_j + \epsilon_j,$$ where $x_j = j$ and $\epsilon_j \sim N(0, x_j)$ for $j=1, \ldots, 100$. Furthermore, the errors $\epsilon_1, \ldots, \epsilon_{100}$ are independent. We fitted a simple linear regression model to the data and added the fitted line to the scatterplot.

## Section 2 Question 1

```{r, data3, fig.cap=""}
# Dataset 3
set.seed(1285)
x3 <- seq(1, 100, length.out=100)
y3 <- 10 + 0.5*x3 + rnorm(100, 0, sqrt(x3))
# Produce a scatterplot
plot(x3, y3)
# Fit a simple linear regression model and add line of best fit to the plot
model3 <- lm(y3 ~ x3)
abline(model3, col="blue", lwd=1.2)
```

The residuals scatter around the zero horizontal line. **But** the variation of the residuals disperses as the fitted values increasing.

```{r ResidualPlot1, fig.cap=""}
# Residual Plot 
plot(model3, 1) 
```

## Section 1, Question 2

```{r, model3_sqrt, fig.cap=""}
plot(x3, sqrt(y3))
model3_sqrt <- lm(sqrt(y3) ~ x3)
abline(model3_sqrt, col="blue", lwd=1.2)
plot(model3_sqrt, 1, lwd=1.2)
```

```{r, model3_log, fig.cap=""}
plot(x3, log(y3))
model3_log <- lm(log(y3) ~ x3)
abline(model3_log, col="blue", lwd=1.2)
plot(model3_log, 1, lwd=1.2)
```

**Comments: Linearity is a more important assumption than homoscedasticity. Thus, if we cannot resolve the heteroscedasticity without the linearity assumption becoming unreasonable, then we choose a model for which the linearity assumption is reasonable, even if it suffers from heteroscedasticity.**

## Section 2 Question 3

Our final artificial dataset is sampled from the model
$$ Y_j \quad = \quad 100 - 20 x_j  + x_j^2 + \epsilon_j,$$
where $\epsilon_1, \ldots, \epsilon_{100}$ are iid $N(0, 400)$.

```{r, data4, fig.cap="Scatterplot of Dataset 4."}
# Dataset 4
set.seed(1066)
x <- seq(0.25, 25, by=0.25)
y <- 100 - 20*x + x^2 + rnorm(100, 0, 20)
# Produce a scatterplot of the dataset
plot(x, y)
```

## Section 2, Question 4

$$ Y_j \quad = \quad \beta_0 + \beta_1 x_{j} + \epsilon_j. $$
```{r linear_model4}
# Fit simple linear regression model and produce residual plot 
plot(x, y)
model4_linear <- lm(y ~ x)
abline(model4_linear, col="blue", lwd=1.2)
plot(model4_linear, 1, lwd=1.2)
```

$$ Y_j \quad = \quad \beta_0 + \beta_1 x_{j} + \beta_2\, x_{j}^2 + \epsilon_j. $$
```{r quadratic_model4}
# Fit quadratic regression model and produce residual plot
model4_quadratic <- lm(y ~ x + I(x^2))
plot(model4_quadratic, 1, lwd=1.2)
```

Only the residual plot of the quadratic regression model is a null plot.

## Section 2, Question 5 and 6

We make use of the function `quadraticPlot` to visualise the quadratic regression model. For element `i` in the `ax` vector, `predict(m, newdata=list(x=ax))` provides $\hat \beta_0 + \hat \beta_1 ax[i] + \hat \beta_2 ax[i]^2$.

```{r, Quadratic, echo=TRUE}
quadraticPlot <- function(x, y){
  # Produce a scatterplot of the data
  plot(x, y, xlab="x", ylab="y", main="Scatterplot")
  # Fit a quadratic regression model
  m <- lm(y ~ x + I(x^2))
  # Create a vector ax of length 101 
  # spanning the range of the explanatory variable
  ax <- seq(min(x), max(x), length.out=101)
  # predict the response for each value in ax
  fitted.curve <- predict(m, newdata=list(x=ax))
  # Fit a curve through the predicted response values and 
  # add to this to the plot
  lines(ax, fitted.curve, col="navy", lwd=2) 
}
```

```{r data3quadratic, fig.cap= ""}
# Apply the quadraticPlot function to the quadratic regression model fitted earlier
quadraticPlot(x, y)
```

This scatterplot shows that the quadratic regression model provides a good fit to the data.

## Section 2, Question 7

Consider
$$ Y_j \quad = \quad \beta_0 + \beta_1 x_{j} + \beta_2\, x_{j}^2 + \epsilon_j, \qquad j = 1, \ldots, n. $$
One unit change of $x_j$ will affect both $\beta_1$ and $\beta_2$. Hence $\beta_1$ and $\beta_2$ should not be interpreted individually.
```{r summary_model4}
coef(model4_quadratic)
```
